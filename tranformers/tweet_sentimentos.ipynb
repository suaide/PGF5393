{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a405a108",
   "metadata": {},
   "source": [
    "\n",
    "# Análise de Sentimento de Tweets em Português \n",
    "**Resumo:** Este notebook mostra, passo a passo, como carregar um dataset de tweets em português, pré‑processar, tunar um modelo Transformer (BERTimbau) com Keras/TensorFlow para classificação de sentimento. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca9495",
   "metadata": {},
   "source": [
    "\n",
    "### Instalação de dependências para Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execute apenas se precisar instalar dependências\n",
    "# Em muitos ambientes já configurados, estas bibliotecas já estarão presentes.\n",
    "# !pip install -q transformers datasets 'tensorflow>=2.10' scikit-learn matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35bd52",
   "metadata": {},
   "source": [
    "\n",
    "### Imports e configurações iniciais\n",
    "\n",
    "Esta célula importa as bibliotecas e define seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TFAutoModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0376b77",
   "metadata": {},
   "source": [
    "\n",
    "### Carregar o dataset `eduagarcia/tweetsentbr_fewshot`\n",
    "\n",
    "Nesta célula carregamos o dataset do Hugging Face Hub. Nesse dataset, só tem 25 twwets de treino. Por isso vamos pegar alguns tweets de teste e mandar para o treino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carrega o dataset do Hugging Face Hub\n",
    "ds = load_dataset('eduagarcia/tweetsentbr_fewshot')\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "\n",
    "# Número de exemplos de teste a mover\n",
    "n_move = len(test_ds) // 2  # metade\n",
    "\n",
    "# Dividimos o test dataset em duas partes\n",
    "test_split = test_ds.train_test_split(test_size=n_move, seed=SEED)\n",
    "\n",
    "# A primeira parte (metade) vai para o treino\n",
    "new_train_ds = Dataset.from_dict({\n",
    "    k: list(train_ds[k]) + list(test_split[\"train\"][k]) for k in train_ds.features\n",
    "})\n",
    "\n",
    "# A segunda metade continua como teste\n",
    "new_test_ds = test_split[\"test\"]\n",
    "\n",
    "# Criamos um novo DatasetDict\n",
    "new_dataset = DatasetDict({\n",
    "    \"train\": new_train_ds,\n",
    "    \"test\": new_test_ds\n",
    "})\n",
    "\n",
    "ds = new_dataset\n",
    "\n",
    "print('Keys do DatasetDict (splits):', ds.keys())\n",
    "split_name = list(ds.keys())[0]\n",
    "print('Colunas disponíveis no split \"{}\":'.format(split_name), ds[split_name].column_names)\n",
    "\n",
    "# Mostra os 3 primeiros exemplos para entendimento do formato\n",
    "print('Exemplos de entrada:')\n",
    "for i in range(3):\n",
    "    print(i+1, ds[split_name][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521270be",
   "metadata": {},
   "source": [
    "\n",
    "### Atribuir os nomes das colunas e mapear os rótulos em números\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80906ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTO = 'sentence'\n",
    "LABEL = 'label'\n",
    "\n",
    "ROTULOS = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "\n",
    "train_df = ds['train'].to_pandas()\n",
    "test_df = ds['test'].to_pandas()\n",
    "\n",
    "train_df[LABEL] = train_df[LABEL].map(ROTULOS)\n",
    "test_df[LABEL] = test_df[LABEL].map(ROTULOS)\n",
    "\n",
    "# converter labels para numpy int32\n",
    "y_train = train_df[LABEL].to_numpy(dtype='int32')\n",
    "y_test = test_df[LABEL].to_numpy(dtype='int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd8a8c",
   "metadata": {},
   "source": [
    "\n",
    "### Só mostra algumas entradas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa618f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Amostra usada para treino:', len(train_df), 'teste:', len(test_df))\n",
    "train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd8c93",
   "metadata": {},
   "source": [
    "\n",
    "### Tokenização com o tokenizer BERTimbau\n",
    "\n",
    "Nesta célula tokenizamos os textos usando `AutoTokenizer`. Configurações importantes:\n",
    "\n",
    "- `max_length=128` (tweets são curtos)\n",
    "- `padding='max_length'` para facilitar a criação de batches estáveis\n",
    "- `truncation=True` para cortar textos muito longos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b6414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'neuralmind/bert-base-portuguese-cased'  # BERTimbau\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_df(df, max_length=128):\n",
    "    enc = tokenizer(list(df[TEXTO]), padding='max_length', truncation=True, max_length=max_length, return_tensors='np')\n",
    "    return enc\n",
    "\n",
    "train_enc = tokenize_df(train_df)\n",
    "test_enc = tokenize_df(test_df)\n",
    "\n",
    "print('Exemplo shapes: input_ids', train_enc['input_ids'].shape, 'attention_mask', train_enc['attention_mask'].shape)\n",
    "print('Labels shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7bb58",
   "metadata": {},
   "source": [
    "\n",
    "### Criar `tf.data.Dataset` a partir dos encodings\n",
    "\n",
    "\n",
    "O modelo espera tensores numéricos (`input_ids`, `attention_mask`) e labels inteiros. Aqui criamos `tf.data.Dataset` com tipo correto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função auxiliar para criar dataset TF\n",
    "def make_tf_dataset(encodings, labels, batch_size=16, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask']\n",
    "        },\n",
    "        labels\n",
    "    ))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000, seed=SEED)\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "batch_size = 16\n",
    "train_ds = make_tf_dataset(train_enc, y_train, batch_size=batch_size, shuffle=True)\n",
    "test_ds = make_tf_dataset(test_enc, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Sanity check: shapes from one batch\n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    print('Batch input_ids shape:', x_batch['input_ids'].shape)\n",
    "    print('Batch attention_mask shape:', x_batch['attention_mask'].shape)\n",
    "    print('Batch labels shape:', y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8545ca1",
   "metadata": {},
   "source": [
    "\n",
    "### Criar e compilar o modelo de classificação (TFAutoModelForSequenceClassification)\n",
    "\n",
    "Aqui carregamos a versão TensorFlow do modelo com uma cabeça de classificação (`num_labels` depende do dataset).Usamos um `learning_rate` pequeno porque estamos fine‑tuning de um modelo pré‑treinado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee4fbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# número de classes detectado a partir do mapeamento\n",
    "num_labels = len(sorted(set(y_train.tolist() + y_test.tolist())))\n",
    "print('Número de classes:', num_labels)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67653921",
   "metadata": {},
   "source": [
    "\n",
    "### Treinamento (fine‑tuning)\n",
    "\n",
    "\n",
    "**Atenção:** em CPU o treinamento pode ser lento. Mantemos `epochs=1` por padrão só para fins de demonstração em sala. Se você tiver GPU disponível, aumente `epochs` maiores. Mas experimente antes porque demora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 1\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34bd7b",
   "metadata": {},
   "source": [
    "\n",
    "### Avaliação e predições de exemplo\n",
    "\n",
    "Geramos métricas e mostramos algumas predições manuais para sentir o comportamento do modelo. Até fazer predição demora se você não tiver uma GPU disponível\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789571fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Avaliar no conjunto de teste\n",
    "results = model.evaluate(test_ds)\n",
    "print('Teste (loss, accuracy):', results)\n",
    "\n",
    "# Função de predição simples para textos novos\n",
    "def predict_text(texts):\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "    logits = model(**enc).logits\n",
    "    probs = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return preds, probs\n",
    "\n",
    "# Exemplos rápidos\n",
    "examples = [\n",
    "    'Amei o atendimento, voltarei sempre!',\n",
    "    'Péssima experiência, demorou demais e o produto veio danificado.',\n",
    "    'O app funciona, mas poderia melhorar.'\n",
    "]\n",
    "\n",
    "preds, probs = predict_text(examples)\n",
    "for t, p, pr in zip(examples, preds, probs):\n",
    "    print('\\nTexto:', t)\n",
    "    print('Predito (id):', int(p), ' — probs:', np.round(pr,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be25c3-30cc-4d85-b898-9d232ed41591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
